{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/scratch/abhimanyu/work/miniconda3/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import argparse\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--model', type = str, default = 'maxvit')\n",
    "    parser.add_argument('--annot_train_prime', type = str, default = 'df_prime_train_features.csv')\n",
    "    parser.add_argument('--annot_test_prime', type = str, default = 'df_prime_test_features.csv')\n",
    "    parser.add_argument('--data_root', type = str, default = '/usr/scratch/abhimanyu/courses/ECE8803_FML/OLIVES')\n",
    "    parser.add_argument('--data_aug', type =int, default = 1)\n",
    "    parser.add_argument('--lr', type = float, default = 5e-4)\n",
    "    parser.add_argument('--weight_decay', type = float, default = 0.1)\n",
    "    parser.add_argument('--momentum', type = float, default = 0.9)\n",
    "    parser.add_argument('--epoch', type = int, default = 50)\n",
    "    parser.add_argument('--batch_size', type = int, default = 1000)\n",
    "    parser.add_argument('--save_pth', type = str, default = '/storage/home/hpaceice1/abambhaniya3/DRSS-Severity-Classification-on-OCT-images/VIT_model_checkpoints/')\n",
    "    parser.add_argument('--load_checkpoint', type = str, default = None)\n",
    "    parser.add_argument(\"--lr-warmup-epochs\", default=5, type=int, help=\"the number of epochs to warmup (default: 5)\")\n",
    "    parser.add_argument(\"--lr-min\", default=1e-5, type=float, help=\"minimum lr of lr schedule (default: 1e-5)\")\n",
    "    # Mixed precision training parameters\n",
    "    parser.add_argument(\"--amp\", action=\"store_true\", help=\"Use torch.cuda.amp for mixed precision training\")\n",
    "    parser.add_argument(\"--lr_scheduler\", type=bool, default = False, help=\"Wethear to turn of LR scheduling or not \") \n",
    "    parser.add_argument(\"--do_batch\", type=int, default = 1, help=\"Wethear to do batching \") \n",
    "    \n",
    "    return parser.parse_known_args()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(device)\n",
    "    args, unkown = parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_dataloader(args, file = 'df_prime_train_features.csv'):\n",
    "    LABELS_Severity = {35: 0,\n",
    "                    43: 0,\n",
    "                    47: 1,\n",
    "                    53: 1,\n",
    "                    61: 2,\n",
    "                    65: 2,\n",
    "                    71: 2,\n",
    "                    85: 2}\n",
    "\n",
    "\n",
    "    annot = pd.read_csv(file)\n",
    "    annot['Severity_Label'] = [LABELS_Severity[drss] for drss in copy.deepcopy(annot['DRSS'].values)]\n",
    "    # root = os.path.expanduser(args.data_root)\n",
    "    # nb_classes=len(np.unique(list(LABELS_Severity.values())))\n",
    "    path_list = annot['Volume_ID'].values\n",
    "\n",
    "    labels = annot['Severity_Label'].values.reshape(-1,1)\n",
    "    # print(labels)\n",
    "    # assert len(path_list) == len(labels)\n",
    "    root = os.path.expanduser(args.data_root)\n",
    "\n",
    "    # get features\n",
    "    # img_volume = np.zeros((len(labels), 49, 224*224), dtype=object)\n",
    "    #img_volume.fill([])\n",
    "    # features = np.zeros((len(labels), 49))\n",
    "    img_volume = []\n",
    "\n",
    "    # for index in range(200):\n",
    "    for index in range(len(path_list)):\n",
    "        # img_volume[index] = []\n",
    "        frames = []\n",
    "        folder_path = root + path_list[index]\n",
    "\n",
    "        # if index%10 == 0  :\n",
    "            # print(f\"Completed {index} images\")\n",
    "        \n",
    "        starting_frame = 10\n",
    "        for i in range(10, 39): \n",
    "            tif = str(i) + '.tif'\n",
    "            png = str(i) + '.png'\n",
    "            \n",
    "            if (os.path.isfile(os.path.join(folder_path, tif))):\n",
    "                img = Image.open(os.path.join(folder_path, tif)).convert(\"L\")\n",
    "            elif (os.path.isfile(os.path.join(folder_path, png))):\n",
    "                img = Image.open(os.path.join(folder_path, png)).convert(\"L\")\n",
    "            else:\n",
    "                img = frames[i -starting_frame - 1]\n",
    "                frames.append(frames[i -starting_frame - 1])\n",
    "                continue\n",
    "            frames.append(np.asarray(img))\n",
    "\n",
    "\n",
    "        img_volume.append(np.array(frames))\n",
    "\n",
    "    print(np.shape(img_volume))\n",
    "    img_volume =np.reshape(img_volume,(len(img_volume), -1)) \n",
    "    # ret = img_volume.reshape((len(img_volume), -1))\n",
    "    print(np.shape(labels))\n",
    "\n",
    "    return img_volume, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/usr/scratch/abhimanyu/courses/ECE8803_FML/DRSS-Severity-Classification-on-OCT-images/data_prep/train_knn.ipynb Cell 4\u001b[0m in \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsynergy3-gpu.ece.gatech.edu/usr/scratch/abhimanyu/courses/ECE8803_FML/DRSS-Severity-Classification-on-OCT-images/data_prep/train_knn.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Load your 3D image dataset\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsynergy3-gpu.ece.gatech.edu/usr/scratch/abhimanyu/courses/ECE8803_FML/DRSS-Severity-Classification-on-OCT-images/data_prep/train_knn.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# Assuming your dataset is loaded into a PyTorch tensor called `data` with shape (num_samples, num_features)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsynergy3-gpu.ece.gatech.edu/usr/scratch/abhimanyu/courses/ECE8803_FML/DRSS-Severity-Classification-on-OCT-images/data_prep/train_knn.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# and the corresponding labels are loaded into a PyTorch tensor called `labels` with shape (num_samples,)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsynergy3-gpu.ece.gatech.edu/usr/scratch/abhimanyu/courses/ECE8803_FML/DRSS-Severity-Classification-on-OCT-images/data_prep/train_knn.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsynergy3-gpu.ece.gatech.edu/usr/scratch/abhimanyu/courses/ECE8803_FML/DRSS-Severity-Classification-on-OCT-images/data_prep/train_knn.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Split the dataset into training and testing sets\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bsynergy3-gpu.ece.gatech.edu/usr/scratch/abhimanyu/courses/ECE8803_FML/DRSS-Severity-Classification-on-OCT-images/data_prep/train_knn.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m X_train, y_train \u001b[39m=\u001b[39m knn_dataloader(args, args\u001b[39m.\u001b[39;49mannot_train_prime )\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsynergy3-gpu.ece.gatech.edu/usr/scratch/abhimanyu/courses/ECE8803_FML/DRSS-Severity-Classification-on-OCT-images/data_prep/train_knn.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m X_test, y_test \u001b[39m=\u001b[39m knn_dataloader(args, args\u001b[39m.\u001b[39mannot_test_prime )\n",
      "\u001b[1;32m/usr/scratch/abhimanyu/courses/ECE8803_FML/DRSS-Severity-Classification-on-OCT-images/data_prep/train_knn.ipynb Cell 4\u001b[0m in \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsynergy3-gpu.ece.gatech.edu/usr/scratch/abhimanyu/courses/ECE8803_FML/DRSS-Severity-Classification-on-OCT-images/data_prep/train_knn.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m png \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(i) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.png\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsynergy3-gpu.ece.gatech.edu/usr/scratch/abhimanyu/courses/ECE8803_FML/DRSS-Severity-Classification-on-OCT-images/data_prep/train_knn.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mif\u001b[39;00m (os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(folder_path, tif))):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bsynergy3-gpu.ece.gatech.edu/usr/scratch/abhimanyu/courses/ECE8803_FML/DRSS-Severity-Classification-on-OCT-images/data_prep/train_knn.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m     img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39;49mopen(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(folder_path, tif))\u001b[39m.\u001b[39;49mconvert(\u001b[39m\"\u001b[39;49m\u001b[39mL\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsynergy3-gpu.ece.gatech.edu/usr/scratch/abhimanyu/courses/ECE8803_FML/DRSS-Severity-Classification-on-OCT-images/data_prep/train_knn.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39melif\u001b[39;00m (os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(folder_path, png))):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsynergy3-gpu.ece.gatech.edu/usr/scratch/abhimanyu/courses/ECE8803_FML/DRSS-Severity-Classification-on-OCT-images/data_prep/train_knn.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m     img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(folder_path, png))\u001b[39m.\u001b[39mconvert(\u001b[39m\"\u001b[39m\u001b[39mL\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/scratch/abhimanyu/work/miniconda3/lib/python3.9/site-packages/PIL/Image.py:913\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    911\u001b[0m         mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mRGBA\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    912\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m mode \u001b[39mor\u001b[39;00m (mode \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m matrix):\n\u001b[0;32m--> 913\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcopy()\n\u001b[1;32m    915\u001b[0m \u001b[39mif\u001b[39;00m matrix:\n\u001b[1;32m    916\u001b[0m     \u001b[39m# matrix conversion\u001b[39;00m\n\u001b[1;32m    917\u001b[0m     \u001b[39mif\u001b[39;00m mode \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mL\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/scratch/abhimanyu/work/miniconda3/lib/python3.9/site-packages/PIL/Image.py:1150\u001b[0m, in \u001b[0;36mImage.copy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1142\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1143\u001b[0m \u001b[39mCopies this image. Use this method if you wish to paste things\u001b[39;00m\n\u001b[1;32m   1144\u001b[0m \u001b[39minto an image, but still retain the original.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[39m:returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[1;32m   1148\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1149\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload()\n\u001b[0;32m-> 1150\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_new(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mim\u001b[39m.\u001b[39;49mcopy())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load your 3D image dataset\n",
    "# Assuming your dataset is loaded into a PyTorch tensor called `data` with shape (num_samples, num_features)\n",
    "# and the corresponding labels are loaded into a PyTorch tensor called `labels` with shape (num_samples,)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, y_train = knn_dataloader(args, args.annot_train_prime )\n",
    "X_test, y_test = knn_dataloader(args, args.annot_test_prime ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train KNN classifier\n",
    "k = 3  # Number of neighbors\n",
    "clf = KNeighborsClassifier(n_neighbors=k)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train KNN classifier\n",
    "k = 5  # Number of neighbors\n",
    "clf = KNeighborsClassifier(n_neighbors=k)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_3 = clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_3)\n",
    "balanced_accuracy = balanced_accuracy_score(y_test,y_pred_3)\n",
    "print(f\"Accuracy: {accuracy:.4f}, Balanced Accuracy: {balanced_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train KNN classifier\n",
    "k = 7  # Number of neighbors\n",
    "clf = KNeighborsClassifier(n_neighbors=k)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_3 = clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_3)\n",
    "balanced_accuracy = balanced_accuracy_score(y_test,y_pred_3)\n",
    "print(f\"Accuracy: {accuracy:.4f}, Balanced Accuracy: {balanced_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train KNN classifier\n",
    "k = 9  # Number of neighbors\n",
    "clf = KNeighborsClassifier(n_neighbors=k)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_3 = clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_3)\n",
    "balanced_accuracy = balanced_accuracy_score(y_test,y_pred_3)\n",
    "print(f\"Accuracy: {accuracy:.4f}, Balanced Accuracy: {balanced_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train KNN classifier\n",
    "k = 11  # Number of neighbors\n",
    "clf = KNeighborsClassifier(n_neighbors=k)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_3 = clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_3)\n",
    "balanced_accuracy = balanced_accuracy_score(y_test,y_pred_3)\n",
    "print(f\"Accuracy: {accuracy:.4f}, Balanced Accuracy: {balanced_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train KNN classifier\n",
    "k = 15  # Number of neighbors\n",
    "clf = KNeighborsClassifier(n_neighbors=k)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_3 = clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_3)\n",
    "balanced_accuracy = balanced_accuracy_score(y_test,y_pred_3)\n",
    "print(f\"Accuracy: {accuracy:.4f}, Balanced Accuracy: {balanced_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
